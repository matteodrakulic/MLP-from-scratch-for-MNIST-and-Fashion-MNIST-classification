# MNIST & Fashion MNIST Classification MLP from scratch (NumPy only)

A pure NumPy implementation of a Multi-Layer Perceptron (MLP) built from scratch. This project demonstrates the fundamentals of deep learning by implementing forward propagation, backpropagation, and gradient descent without using high-level frameworks like PyTorch or TensorFlow.

The model is trained and compared on two classic datasets: **MNIST** (Digits) and **Fashion MNIST** (Clothing).

## ðŸš€ Features
- **NumPy only**: No auto-differentiation engines; all gradients are computed manually.
- **Custom Layers**: `Linear`, `ReLU`, and `SoftmaxCrossEntropy` implemented from scratch.
- **Dynamic Architecture**: Easily configurable number of hidden layers and neurons.
- **Comparison**: Visualizes performance differences between MNIST and Fashion MNIST.

## ðŸ§  Mathematical Derivation

The core of this project is the manual implementation of Backpropagation. Here are the gradients derived for the linear layer and loss function.

### 1. Notation
- $X$: Input batch of shape $(N, D_{in})$
- $W$: Weights of shape $(D_{in}, D_{out})$
- $b$: Bias of shape $(D_{out},)$
- $Z = XW + b$: Linear output
- $A = \sigma(Z)$: Activation (ReLU)
- $L$: Loss function (Cross Entropy)

### 2. Linear Layer Gradients
We need to find $\frac{\partial L}{\partial W}$, $\frac{\partial L}{\partial b}$, and $\frac{\partial L}{\partial X}$ (to pass to the previous layer).

Given the incoming gradient $\delta = \frac{\partial L}{\partial Z}$ from the next layer:

$$
\frac{\partial L}{\partial W} = X^T \cdot \delta
$$

$$
\frac{\partial L}{\partial b} = \sum_{i=1}^{N} \delta_i \quad \text{(Sum over batch)}
$$

$$
\frac{\partial L}{\partial X} = \delta \cdot W^T
$$

### 3. Softmax + Cross Entropy Gradient
Combining Softmax and Cross Entropy simplifies the gradient calculation significantly.
Let $p$ be the predicted probability vector and $y$ be the one-hot encoded true label.

$$
\frac{\partial L}{\partial z_i} = p_i - y_i
$$

In code (efficient batch implementation):
```python
grad = probs.copy()
grad[np.arange(batch_size), y] -= 1
grad /= batch_size
```

## ðŸ“Š Results

### Training Performance
| Dataset | Epochs | Training Time (approx) | Final Test Accuracy |
|---------|--------|------------------------|---------------------|
| MNIST | 10 | ~2.5s / epoch | **~97.5%** |
| Fashion MNIST | 10 | ~3.0s / epoch | **~88.0%** |

### Loss & Accuracy Curves
The plot below (generated by `plot.py`) shows the learning progress. MNIST is significantly easier to learn than Fashion MNIST, converging faster to a higher accuracy.

![Comparison Plot](comparison_plot.png)

## ðŸ“‚ Project Structure
- `layers.py`: Implementation of Linear, ReLU, and Softmax layers.
- `model.py`: The `MLP` class that stacks layers and manages forward/backward passes.
- `trainer.py`: Handles the training loop, batching, and evaluation.
- `plot.py`: Visualization utilities using Matplotlib.
- `main.py`: Entry point for loading data and running the comparison.
